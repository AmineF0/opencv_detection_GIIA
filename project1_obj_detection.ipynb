{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>this project is created by YOUNESS AABIBI<h3>\n",
    "<h3>the goal of this project is to detect objects in images using only opencv<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>task1: detect objects in images<h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>method 1 : using cv2.canny()<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read img\n",
    "img = cv2.imread('imgs/creeper.webp')\n",
    "# showing img\n",
    "cv2.imshow('image', img)\n",
    "# wait for key to close\n",
    "cv2.waitKey(0)\n",
    "# destroy all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the img to black and white such that the edges of an object is in white and rest is black\n",
    "# to convert this img we will us canny fct of opencv\n",
    "canny = cv2.Canny(img, 20, 200)\n",
    "cv2.imshow('image2', canny)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_edgs_of_object(canny):\n",
    "    \"\"\"\n",
    "        this fct detects the edges of an object in an img we check if the pixel is white or black then we get the max and min of x and y \n",
    "        so we can draw a rectangle around the object\n",
    "        param: canny: the result of canny fct\n",
    "        return: max_x,min_x,max_y,min_y: the max and min of x and y\n",
    "    \"\"\"\n",
    "    max_x,min_x,max_y,min_y = [0,10000000,0,10000000]\n",
    "    height,width = canny.shape\n",
    "    print(canny.shape)\n",
    "    for i in range(0, height):             \n",
    "        for j in range(0, (width)):\n",
    "            if canny[i,j]==255:\n",
    "                if j>=max_x:\n",
    "                    max_x=j\n",
    "                if j<=min_x:\n",
    "                    min_x=j\n",
    "                if i>=max_y:\n",
    "                    max_y=i\n",
    "                if i<=min_y:\n",
    "                    min_y=i\n",
    "    return max_x,min_x,max_y,min_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1290, 860)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    the problem with this methode is that we can detect just one object in the img\n",
    "\"\"\"\n",
    "max_x,min_x,max_y,min_y = detect_edgs_of_object(canny)\n",
    "cv2.rectangle(img,(min_x,min_y), (max_x, max_y),(0,255,0),2)      \n",
    "cv2.imshow('img',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>method 2 : using cv2.canny() and cv2.findContours()<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "[56672, 54312, 35280]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    this methode draw a contour around the objects in the img\n",
    "    to work with this methode we need to convert the img to canny img or using the threshold fct\n",
    "    but i will use the canny img becaus it is more clear and also it is more accurate\n",
    "    \n",
    "\"\"\"\n",
    "# read img\n",
    "img = cv2.imread('imgs/phone.webp')\n",
    "# convert to canny img \n",
    "img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "\n",
    "# equalize the histogram of the Y channel\n",
    "img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n",
    "\n",
    "# convert the YUV image back to RGB format\n",
    "img = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n",
    "blurred = cv2.GaussianBlur(img, (15, 15), 0)\n",
    "hsv = cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV)\n",
    "blurred = cv2.GaussianBlur(hsv, (9, 9), 0)\n",
    "canny = cv2.Canny(blurred, 30, 200)\n",
    "# we can clear the noise in the img by using the Dilation fct in opencv\n",
    "kernel = np.ones((1, 1), np.uint8)\n",
    "# dilate the img\n",
    "img_dilate = cv2.erode(canny, kernel, iterations=1)\n",
    "# find the contours\n",
    "contours, hierarchy = cv2.findContours(img_dilate, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "# draw the contours\n",
    "print(len(contours))\n",
    "img_copy=img.copy()\n",
    "maxx=[0,0,0]\n",
    "maxy=[0,0,0]\n",
    "maxw=[0,0,0]\n",
    "maxh=[0,0,0]\n",
    "j=0\n",
    "maxarea=[0,0,0]\n",
    "for i,c in enumerate(contours):\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    for k in range(len(maxarea)):\n",
    "        if w*h>maxarea[k]:\n",
    "            maxarea[k]=w*h\n",
    "            maxx[k]=x\n",
    "            maxy[k]=y\n",
    "            maxw[k]=w\n",
    "            maxh[k]=h\n",
    "            break\n",
    "print(maxarea)\n",
    "for i in range(len(maxarea)):\n",
    "    cv2.rectangle(img, (maxx[i]+20,maxy[i]+20), (maxx[i]+maxw[i], maxy[i]+maxh[i]), (255, 255, 0), 2)\n",
    "# show the img\n",
    "cv2.imshow('img', img)\n",
    "cv2.imshow('img_dilate', img_dilate)\n",
    "cv2.imshow('canny', canny)\n",
    "cv2.imshow('img_copy', img_copy)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>method 3: using cascade classifier<h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    to use cascade classifier we need to download the xml file from the internet\n",
    "    the down side of this method is that we need xml file for each object we want to detect\n",
    "\"\"\"\n",
    "img = cv2.imread('imgs/stop.jpg')\n",
    "# read the xml file\n",
    "cascade_classifier = cv2.CascadeClassifier('haarcascade/stop_data.xml')\n",
    "# find data\n",
    "data = cascade_classifier.detectMultiScale(img, 1.1, 5)\n",
    "# draw rectangle around the object\n",
    "if len(data):\n",
    "    for (x, y, w, h) in data:\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "# show the img\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>method 4: using cv2.dnn method<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    this methode is the best one to detect objects in an img because it uses deep learning model to detect objects\n",
    "    dnn stands for deep neural network\n",
    "    so first thing we need to do is to download the pre-trained model to detect objects (you will find the model inside  folder caled model)  \n",
    "    the model's size is 13 MB so if you dont want to download it you can skip this method\n",
    "\"\"\"\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('imgs/phone.webp')\n",
    "cv2.imshow('Output', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn_DetectionModel('model/frozen_inference_graph.pb', 'model/ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "< cv2.dnn.Model 00000219BED93630>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.setInputSize(320 , 230)\n",
    "net.setInputScale(1.0 / 127.5)\n",
    "net.setInputMean((127.5, 127.5, 127.5))\n",
    "net.setInputSwapRB(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "classIds, confs, bbox = net.detect(img, confThreshold=0.5)\n",
    "index = cv2.dnn.NMSBoxes(bbox, confs, 0.5, 0.4)\n",
    "\n",
    "for i, box in enumerate( bbox):\n",
    "    if i in index:\n",
    "        cv2.rectangle(img, box, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "\n",
    "cv2.imshow('Output', img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>task 2 : face detection in livecam using opencv<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    to detect faces using opencv we can use CascadeClassifier fct \n",
    "\"\"\"\n",
    "cap = cv2.VideoCapture(0)\n",
    "# read the xml file\n",
    "cascade_face = cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_alt.xml')\n",
    "while True:\n",
    "    # read frame\n",
    "    ret, frame = cap.read()\n",
    "    # flip the frame so that it is not the mirror view\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # detecting the face in the frame\n",
    "    found = cascade_face.detectMultiScale(frame, 1.1,5)\n",
    "    # if face is found draw rectangle around the face\n",
    "    if len(found):\n",
    "        # draw a rectangle around the face  \n",
    "        for (x,y,widht,height) in found:\n",
    "            cv2.rectangle(frame, (x,y), (x+widht, y+height), (0,255,0), 2)\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> task 3: face and hand detaction <h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    to detect face and hands we will use cv2.CascadeClassifier fct but there is problem with that the hand is not detected well\n",
    "\"\"\"\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# read the xml file\n",
    "cascade_face = cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_alt.xml')\n",
    "cascade_hand = cv2.CascadeClassifier(\"haarcascade/1256617233-1-haarcascade_hand.xml\")\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    face_found = cascade_face.detectMultiScale(gray, 1.1,5)\n",
    "    hand_found = cascade_hand.detectMultiScale(gray, 1.1,5)\n",
    "    if len(face_found):\n",
    "        for (x,y,widht,height) in face_found:\n",
    "            cv2.rectangle(frame, (x,y), (x+widht, y+height), (255,255,0), 2)\n",
    "    if len(hand_found):\n",
    "        for (x,y,widht,height) in hand_found:\n",
    "            cv2.rectangle(frame, (x,y), (x+widht, y+height), (0,255,0), 2)\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    to fix this problem i tried to mask the frame \n",
    "\"\"\"\n",
    "#reading the input webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "# hand_cascade = cv2.CascadeClassifier('./haarcascade/haarcascade_hand.xml')\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_alt.xml')\n",
    "palm_cascade = cv2.CascadeClassifier('./haarcascade/1256617233-1-haarcascade_hand.xml')\n",
    "while True :\n",
    "    ret,frame = cap.read()\n",
    "    frame = cv2.flip(frame,1)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    min_YCrCb = np.array([0,133,77],np.uint8)\n",
    "    max_YCrCb = np.array([235,173,127],np.uint8)\n",
    "    imageYCrCb = cv2.cvtColor(frame,cv2.COLOR_BGR2YCR_CB)\n",
    "    skinRegionYCrCb = cv2.inRange(imageYCrCb,min_YCrCb,max_YCrCb)\n",
    "    skinYCrCb = cv2.bitwise_and(frame, frame, mask = skinRegionYCrCb)\n",
    "    #Erosion\n",
    "    kernel = np.ones((1,1), np.uint8)\n",
    "\n",
    "    erode_image = cv2.erode(skinYCrCb,kernel, iterations=1)\n",
    "    \n",
    "    cv2.imshow('image', skinYCrCb)\n",
    "    found_palm = palm_cascade.detectMultiScale(skinYCrCb, 1.1,25)\n",
    "    found_face = face_cascade.detectMultiScale(frame, 1.1,5)\n",
    "    \n",
    "    if len(found_palm):\n",
    "        for (x,y,widht,height) in found_palm:\n",
    "            cv2.rectangle(frame, (x,y), (x+widht, y+height), (0,255,0), 2)\n",
    "    if len(found_face):\n",
    "        for (x,y,widht,height) in found_face:\n",
    "            cv2.rectangle(frame, (x,y), (x+widht, y+height), (255,255,0), 2)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>task 4 draw with your hand<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    this fct doesnt prforme well because the hand haarcascaed is bad \\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "hand_cascade = cv2.CascadeClassifier('./haarcascade/1256617233-1-haarcascade_hand.xml')\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_alt.xml')\n",
    "points = []\n",
    "blank = np.zeros([512,512,3],dtype=np.uint8)\n",
    "blank.fill(0)\n",
    "fx,fy,fw,fh = [0,0,0,0]\n",
    "while True :\n",
    "    ret,frame = cap.read()\n",
    "    frame = cv2.flip(frame,1)\n",
    "    min_YCrCb = np.array([0,133,77],np.uint8)\n",
    "    max_YCrCb = np.array([235,173,127],np.uint8)\n",
    "    imageYCrCb = cv2.cvtColor(frame,cv2.COLOR_BGR2YCR_CB)\n",
    "    skinRegionYCrCb = cv2.inRange(imageYCrCb,min_YCrCb,max_YCrCb)\n",
    "    skinYCrCb = cv2.bitwise_and(frame, frame, mask = skinRegionYCrCb)\n",
    "    #Erosion\n",
    "    kernel = np.ones((2,2), np.uint8)\n",
    "\n",
    "    erode_image = cv2.erode(skinYCrCb,kernel, iterations=1)\n",
    "    found = hand_cascade.detectMultiScale(erode_image, 1.1,25)\n",
    "    found_face = face_cascade.detectMultiScale(frame, 1.1,5)\n",
    "    if len(found):\n",
    "        \n",
    "        if len(found_face):\n",
    "            fx,fy,fw,fh = found_face[0]\n",
    "        for (x,y,widht,height) in found:\n",
    "            cv2.rectangle(frame, (x,y), (x+widht, y+height), (0,255,0), 2)\n",
    "            center = (x + widht//2, y + height//2)\n",
    "            cv2.circle(frame, center, 20, (255, 0, 0), 2)\n",
    "            if center[0]>= fx and center[0] <= fx+fw and center[1] >= fy and center[1] <= fy+fh:\n",
    "                points=[] \n",
    "            elif len(points) <2:\n",
    "                points.append(center)\n",
    "            else:\n",
    "                for i in range(1,len(points)):\n",
    "                    cv2.line(blank, points[i-1], points[i], (0, 0, 255), 2)\n",
    "                points.append(center)\n",
    "    if len(found_face):\n",
    "        for (x,y,widht,height) in found_face:\n",
    "            cv2.rectangle(frame, (x,y), (x+widht, y+height), (255,0,0), 2)\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.imshow('image', blank)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\"\"\"\n",
    "    this fct doesnt prforme well because the hand haarcascaed is bad \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "< cv2.dnn.Net 000001BCFFC82F90>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = cv2.dnn.readNet(\"model/cross-hands.weights\",\"model/cross-hands.cfg\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_layers(net):\n",
    "    \n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers=[]\n",
    "    for i in net.getUnconnectedOutLayers():\n",
    "        output_layers.append(layer_names[i-1])\n",
    "\n",
    "    return output_layers\n",
    "\n",
    "# function to draw bounding box on the detected object with class name\n",
    "def draw_bounding_box(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
    "\n",
    "    color = (255,255)\n",
    "\n",
    "    cv2.rectangle(img, (x,y), (x_plus_w,y_plus_h), color, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    to detect hands we will use mediapipe library\\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "this model is so heavy so if you dont want to download it you can skip this method\n",
    "this method just detect hands\n",
    "\"\"\"\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True :\n",
    "    ret,img = cap.read()\n",
    "    img = cv2.flip(img,1)\n",
    "    Height, Width, _ = img.shape\n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (416,416), (0,0,0), True, crop=False)\n",
    "\n",
    "\n",
    "    model.setInput(blob)\n",
    "    \n",
    "    # run inference through the network\n",
    "    # and gather predictions from output layers\n",
    "    outs = model.forward(get_output_layers(model))\n",
    "    \n",
    "    # initialization\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    conf_threshold = 0.5\n",
    "    nms_threshold = 0.4\n",
    "\n",
    "    # for each detetion from each output layer \n",
    "    # get the confidence, class id, bounding box params\n",
    "    # and ignore weak detections (confidence < 0.5)\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * Width)\n",
    "                center_y = int(detection[1] * Height)\n",
    "                w = int(detection[2] * Width)\n",
    "                h = int(detection[3] * Height)\n",
    "                x = center_x - w / 2\n",
    "                y = center_y - h / 2\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, w, h])\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "    img_copy = img.copy()\n",
    "    # go through the detections remaining\n",
    "    # after nms and draw bounding box\n",
    "    for i in indices:\n",
    "        box = boxes[i]\n",
    "        x = box[0]\n",
    "        y = box[1]\n",
    "        w = box[2]\n",
    "        h = box[3]\n",
    "        draw_bounding_box(img_copy, class_ids[i], confidences[i], round(x), round(y), round(x+w), round(y+h))\n",
    "    cv2.imshow(\"object detection\", img_copy)\n",
    "    cv2.imshow(\"original\", img)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\"\"\"\n",
    "    to detect hands we will use mediapipe library\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
